{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6c6773-90e3-4e80-869f-116127798636",
   "metadata": {},
   "source": [
    "Forward Propagation purely implemented in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac84127-90b8-4219-b6e2-e6a141463f12",
   "metadata": {},
   "source": [
    "\n",
    "First we implement the code with complete for loops, without using any vectorization or library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9c578c-1e98-47ed-bbb6-f6983328c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21882f62-022f-43cf-9cd6-bda02d86fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our input vector with two features\n",
    "x = np.array([200, 17])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28da4931-88f1-4d79-9709-ed2a562d3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1 has 3 neurons so it can be modeled as,\n",
    "\n",
    "#unit_1 of Layer 1\n",
    "w1_1 = np.array([1,2])\n",
    "b1_1 = np.array([-1])\n",
    "z1_1 = np.dot(w1_1,x)+b1_1\n",
    "a1_1 = 1/(1+np.exp(-z1_1))\n",
    "\n",
    "#unit_2 of Layer 1\n",
    "w1_2 = np.array([-3,4])\n",
    "b1_2 = np.array([1])\n",
    "z1_2 = np.dot(w1_2,x)+b1_2\n",
    "a1_2 = 1/(1+np.exp(-z1_2))\n",
    "\n",
    "#unit_3 of Layer 1\n",
    "w1_3 = np.array([5,-6])\n",
    "b1_3 = np.array([2])\n",
    "z1_3 = np.dot(w1_3,x)+b1_3\n",
    "a1_3 = 1/(1+np.exp(-z1_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "105e8c64-9da2-4339-bea4-d07abc19f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the NN is: [0.99330715]\n"
     ]
    }
   ],
   "source": [
    "#Layer 2 is output layer with one unit\n",
    "# a1 with three values is input to this layer so lets first convert it into a vector\n",
    "\n",
    "a1 = np.array([a1_1 , a1_2, a1_3])\n",
    "\n",
    "#NOw lets model the unit of the final output layer\n",
    "w2_1 = np.array([-7,8,9])\n",
    "b2_1 = np.array([3])\n",
    "z2_1 = np.dot(w2_1,a1)+b2_1\n",
    "a2_1 = 1/(1+np.exp(-z2_1))\n",
    "#Now lets print the out put of the final layer\n",
    "print(f'The output of the NN is: {a2_1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ee7b5-ea7e-4a46-b7dc-705196bceb6e",
   "metadata": {},
   "source": [
    "**Now lets implement the given forward propagtion network with Numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d59c1cf-0457-4887-9ba1-4ad804ae5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all weights of layer 1 in matrix\n",
    "W1 = np.array([[1, -3, 5],[2, 4, 6]])\n",
    "b1 = np.array([-1, 1, 2])\n",
    "#input feature vector\n",
    "a_in = np.array([200, 17])\n",
    "#defining parameters of layer 2\n",
    "W2= np.array([-7, 8, 9])\n",
    "b2= np.array([3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d19c168-d973-4c5a-8b40-f2546ee2296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute layer parameters\n",
    "def dense(W, b, a_in):\n",
    "    units = W.shape[1]\n",
    "    print(\"total units in this layer are :\", units)\n",
    "    a_out = np.zeros(units)\n",
    "    for j in range(units):\n",
    "        w = W[:,j]\n",
    "        z = np.dot(W[:,j], a_in)+ b[j]\n",
    "        a_out[j] = 1/(1+np.exp(-z))\n",
    "        print(a_out[j])\n",
    "    return a_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e95554a9-808f-4766-95fd-2f57f0f039ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total units in this layer are : 3\n",
      "1.0\n",
      "2.4526191187752155e-231\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a1= np.zeros(W1[1])\n",
    "a1= dense(W1, b1, a_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5d0df865-61b9-4988-a8a7-fa6cd62ac0d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a1\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(a1)\n\u001b[1;32m      2\u001b[0m a1\n\u001b[0;32m----> 3\u001b[0m a2\u001b[38;5;241m=\u001b[39m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m, in \u001b[0;36mdense\u001b[0;34m(W, b, a_in)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdense\u001b[39m(W, b, a_in):\n\u001b[0;32m----> 3\u001b[0m     units \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal units in this layer are :\u001b[39m\u001b[38;5;124m\"\u001b[39m, units)\n\u001b[1;32m      5\u001b[0m     a_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(units)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "a1= np.transpose(a1)\n",
    "a1\n",
    "a2=dense(W2, b2, a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a40a9-2b1b-408e-85ce-55f1d2b944b6",
   "metadata": {},
   "source": [
    "NOw lets with vectorized numpy implementation without using any for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27dda7eb-4663-4adf-8017-40eb70435d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense1 (A_in, W, b):\n",
    "    z= np.matmul(A_in, W)+b\n",
    "    A_out = np.matrix(1/(1+np.exp(-z)))\n",
    "    print (A_out)\n",
    "    return np.transpose(A_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dc1751d1-aea6-43a2-b52d-41426d75df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+000 2.45261912e-231 1.00000000e+000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.00000000e+000],\n",
       "        [2.45261912e-231],\n",
       "        [1.00000000e+000]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1(x,W1,b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c0b746dd-bf33-4244-ba6d-8f7f30df5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train neural network in tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb518a7b-776f-4c3c-9eca-42d5de2ac5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([ Dense(units=25, activation = 'sigmoid'),\n",
    "                    Dense(units=15, activation = 'sigmoid'),\n",
    "                    Dense(units=1, activation =  'sigmoid'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "01476ceb-4f00-4f17-86a8-870930f56c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f79e8d9b-4065-4938-8cee-fb6dfa217c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= BinaryCrossentropy)\n",
    "\n",
    "#by default it will compile the model using gradient descent, if we want to choose another optimizer such as adam. then we will write the code as,\n",
    "#model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= 1e-3), loss= SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "#for multiple classes we use Softmax cost function which is SparseCategoricalCrossentropy\n",
    "#for numerical cross erros avoding we use from_logits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0f4e0226-30f3-4435-9a4f-53042c2d18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=x\n",
    "#X = np.append(X, [200,300,7])\n",
    "#print(X)\n",
    "#Y= np.array([1,0, 1,1, 0])\n",
    "#model.fit(X,Y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3240f7-bb0c-4c75-a904-28f623c420cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
